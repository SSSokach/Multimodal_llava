<h2 align="center"> <a href="https://arxiv.org/abs/2311.10122">Multimodel-LLaVA: åŸºäºVideo-LLaVAä¸LanguageBindå¯¹é½æ–‡æœ¬ä¸ä¸ƒä¸ªæ¨¡æ€</a></h2>
<h5 align="center"> å¦‚æœä½ å–œæ¬¢æˆ‘ä»¬çš„é¡¹ç›®ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªâ­</h2>



## ğŸ˜® äº®ç‚¹

æˆ‘ä»¬å°†videollavaçš„èƒ½åŠ›æ‰©å±•åˆ°å…«ç§æ¨¡å¼
- æ–‡æœ¬
- å›¾ç‰‡
- è§†é¢‘
- éŸ³é¢‘
- æ·±åº¦
- çƒ­
- æ°´å£°
- ç”µç£æ³¢


## ğŸ› ï¸ ç¯å¢ƒé…ç½®
* Python >= 3.10
* Pytorch >= 2.0.1
* CUDA Version >= 11.7
* Install required packages:
```bash
git clone https://github.com/SSSokach/Multimodal_llava.git
cd Video-LLaVA
conda create -n mllava python=3.11
conda activate mllava

# install torch which match your cuda
# show example torch2.1.2+cuda12.1
conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia

pip install --upgrade pip  # enable PEP 660 support
pip install -e .
pip install -e ".[train]"

pip install decord opencv-python git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d
pip install flash-attn --no-build-isolation
# if flash-attn appear error
# find the verson match your environment, download from https://github.com/Dao-AILab/flash-attention/releases , and install it manually
# pip install /download_path/flash_attn-2.1.0+cu121torch2.1cxx11abiFALSE-cp311-cp311-linux_x86_64.whl

# if torchaudio>2.1.0  maybe appear Error:' No audio backend is available.'
# download backend: sox/soundfile
# conda install -c conda-forge sox
```

## ğŸ—ï¸ æ•°æ® & è®­ç»ƒ & æ¨ç†

### æ•°æ®


è¯·å‡†å¤‡è‡ªå·±çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®ã€‚ç›®å½•`./data`é‡Œæä¾›äº†ä¸€äº›å‚è€ƒæ ·ä¾‹ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªå›¾ç‰‡æ¨¡æ€æŒ‡ä»¤æ•°æ®çš„æ ·ä¾‹:

```json
[
    {
        "multimodal_mode": "image",
        "image": "data_file/image.jpg",
        "conversations": [
            {
                "from": "human",
                "value": "<image>\nè¯·è¯¦ç»†æè¿°ç»™å®šçš„å›¾ç‰‡ã€‚"
            },
            {
                "from": "gpt",
                "value": "è¿™å¼ ç…§ç‰‡æ•æ‰åˆ°ä¸€åæ»‘æ¿è¿åŠ¨å‘˜åœ¨è¡¨æ¼”ç‰¹æŠ€æ—¶è·³ä¸Šé‡‘å±æ æ†çš„åŠ¨ä½œã€‚ç°åœºè¿˜æœ‰å…¶ä»–äººã€‚å‡ è¾†è½¦åŒ…å›´äº†æ»‘æ¿æ‰‹ç»ƒä¹ æŠ€å·§çš„åŒºåŸŸã€‚å¯ä»¥çœ‹åˆ°äº”è¾†è½¦åœåœ¨ä»–èº«åã€‚å¯ä»¥çœ‹åˆ°ç¬¬äºŒä¸ªæ»‘æ¿è¢«å³ä¾§çš„äººéª‘ç€ã€‚"
            }
        ]
    }
    {
        ...
    },
    {
        ...
    }
]
```

- `multimodal_mode`  å¯ä»¥å¡« `imageï¼Œvideoï¼Œaudioï¼Œdepthï¼Œthermalï¼Œnone`,  `none` ä»£è¡¨è¯¥æ¡æ•°æ®æ˜¯æ–‡æœ¬æ•°æ®ã€‚
- `image` å¡«æŒ‡ä»¤å¯¹åº”çš„å›¾ç‰‡æ–‡ä»¶åœ°å€ã€‚
- `conversations` æ”¯æŒå•è½®å¯¹è¯ä¸å¤šè½®å¯¹è¯ã€‚

### è®­ç»ƒ

```bash
# prepare model weight
python -m videollava.split_model_utils

# training
bash scripts/v1_5/finetune_continue_sft.sh
```

### æ¨ç†

```bash
bash scripts/v1_5/cli_auto_inferrence.sh
```



## ğŸ‘ è‡´è°¢

* æˆ‘ä»¬åŸºäº[LLaVA](https://github.com/haotian-liu/LLaVA) ä¸ [Video-LLaVA](https://github.com/PKU-YuanGroup/Video-LLaVA) è¿™ä¸¤ä¸ªä»£ç åº“å®ç°çš„ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—ä¼˜ç§€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚

## ğŸ”’ è®¸å¯
- è¿™ä¸ªé¡¹ç›®çš„å¤§éƒ¨åˆ†æ˜¯åœ¨[license](https://github.com/PKU-YuanGroup/Video-LLaVA/blob/main/LICENSE)æ–‡ä»¶ä¸­æ‰¾åˆ°çš„Apache 2.0è®¸å¯è¯ä¸‹å‘å¸ƒçš„ã€‚